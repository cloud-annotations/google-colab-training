{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "object-detection.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cloud-annotations/google-colab-training/blob/master/object_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZSL793i7KuM",
        "colab_type": "text"
      },
      "source": [
        "# Setup\n",
        "The only thing you **NEED** to change is `ANNOTATIONS_FOLDER`.\n",
        "\n",
        "You can easily create your own annotations for free at [Cloud Annotations](https://cloud.annotations.ai).\n",
        "\n",
        "### Steps\n",
        "1. Start a new project\n",
        "1. Choose **`localization`** as the project type\n",
        "1. Label your images and videos\n",
        "1. Export an annotation folder by clicking `File > Export as Create ML`\n",
        "1. Unzip the folder and upload it to Google Drive\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVPzEKoLuEHy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "################################################################################\n",
        "# Things to change:\n",
        "ANNOTATIONS_FOLDER = 'THE_NAME_OF_YOUR_FOLDER_IN_GOOGLE_DRIVE'\n",
        "NUM_TRAIN_STEPS = 500\n",
        "################################################################################\n",
        "\n",
        "import os\n",
        "GOOGLE_DRIVE_MOUNT    = '/content/drive'\n",
        "ANNOTATIONS_PATH      = os.path.join(GOOGLE_DRIVE_MOUNT, 'My Drive', ANNOTATIONS_FOLDER)\n",
        "ANNOTATIONS_JSON_PATH = os.path.join(ANNOTATIONS_PATH, 'annotations.json')\n",
        "\n",
        "CHECKPOINT_PATH = '/content/checkpoint'\n",
        "OUTPUT_PATH     = '/content/output'\n",
        "EXPORTED_PATH   = '/content/exported'\n",
        "DATA_PATH       = '/content/data'\n",
        "\n",
        "LABEL_MAP_PATH    = os.path.join(DATA_PATH, 'label_map.pbtxt')\n",
        "TRAIN_RECORD_PATH = os.path.join(DATA_PATH, 'train.record')\n",
        "VAL_RECORD_PATH   = os.path.join(DATA_PATH, 'val.record')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XINCKkPshgz",
        "colab_type": "text"
      },
      "source": [
        "# Install the TensorFlow Object Detection API\n",
        "In order to use the TensorFlow Object Detection API, we need to clone it's GitHub Repo.\n",
        "\n",
        "### Dependencies\n",
        "Most of the dependencies required come preloaded in Google Colab. The only additional package we need to install is TensorFlow.js, which is used for converting our trained model to a model that is compatible for the web.\n",
        "\n",
        "### Protocol Buffers\n",
        "The TensorFlow Object Detection API relies on what are called `protocol buffers` (also known as `protobufs`). Protobufs are a language neutral way to describe information. That means you can write a protobuf once and then compile it to be used with other languages, like Python, Java or C.\n",
        "\n",
        "The `protoc` command used below is compiling all the protocol buffers in the `object_detection/protos` folder for Python.\n",
        "\n",
        "### Environment\n",
        "To use the object detection api we need to add it to our `PYTHONPATH` along with `slim` which contains code for training and evaluating several widely used Convolutional Neural Network (CNN) image classification models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o33_jgwGm3NV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "!cd /content\n",
        "!git clone https://github.com/tensorflow/models.git\n",
        "!pip install --no-deps tensorflowjs==1.4.0\n",
        "\n",
        "%cd /content/models/research\n",
        "!protoc object_detection/protos/*.proto --python_out=.\n",
        "\n",
        "pwd = os.getcwd()\n",
        "os.environ['PYTHONPATH'] += f':{pwd}:{pwd}/slim'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wS1ZDbJ660Wv",
        "colab_type": "text"
      },
      "source": [
        "# Test the setup\n",
        "If everything was set up properly and nothing went wrong, we should be able to run this command."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iM8sOHwL64Rp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python object_detection/builders/model_builder_test.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-DHE8xTssqT",
        "colab_type": "text"
      },
      "source": [
        "# Mount Google Drive\n",
        "In order to use files from Google Drive we need to mount it to Colab.\n",
        "\n",
        "When running this command for the first time it will ask you to open a link to accept permissions. After doing so, it will give you an authorization code that you can copy and paste below to complete the mounting process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "koKy90U5KAnh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(GOOGLE_DRIVE_MOUNT)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISX8k0TfdDHj",
        "colab_type": "text"
      },
      "source": [
        "# Generate a Label Map\n",
        "One piece of data the Object Detection API needs is a label map protobuf. The label map associates an integer id to the text representation of the label. The ids are indexed by 1, meaning the first label will have an id of 1 not 0.\n",
        "\n",
        "Here is an example of what a label map looks like:\n",
        "```\n",
        "item {\n",
        "  id: 1\n",
        "  name: 'Cat'\n",
        "}\n",
        "\n",
        "item {\n",
        "  id: 2\n",
        "  name: 'Dog'\n",
        "}\n",
        "\n",
        "item {\n",
        "  id: 3\n",
        "  name: 'Gold Fish'\n",
        "}\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJsKCG3UdDsn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "# Get a list of labels from the annotations.json\n",
        "labels = {}\n",
        "with open(ANNOTATIONS_JSON_PATH) as f:\n",
        "  annotations = json.load(f)\n",
        "  labels = {l['label'] for a in annotations for l in a['annotations']}\n",
        "\n",
        "# Create a file named label_map.pbtxt\n",
        "os.makedirs(DATA_PATH, exist_ok=True)\n",
        "with open(LABEL_MAP_PATH, 'w') as f:\n",
        "  # Loop through all of the labels and write each label to the file with an id\n",
        "  for idx, label in enumerate(labels):\n",
        "    f.write('item {\\n')\n",
        "    f.write(\"\\tname: '{}'\\n\".format(label))\n",
        "    f.write('\\tid: {}\\n'.format(idx + 1)) # indexes must start at 1\n",
        "    f.write('}\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "siRNKiuvsz25",
        "colab_type": "text"
      },
      "source": [
        "# Generate TFRecords\n",
        "The TensorFlow Object Detection API expects our data to be in the format of TFRecords.\n",
        "\n",
        "The TFRecord format is a collection of serialized feature dicts, one for each image, looking something like this:\n",
        "```\n",
        "{\n",
        "  'image/height': 1800,\n",
        "  'image/width': 2400,\n",
        "  'image/filename': 'image1.jpg',\n",
        "  'image/source_id': 'image1.jpg',\n",
        "  'image/encoded': ACTUAL_ENCODED_IMAGE_DATA_AS_BYTES,\n",
        "  'image/format': 'jpeg',\n",
        "  'image/object/bbox/xmin': [0.7255949630314233, 0.8845598428835489],\n",
        "  'image/object/bbox/xmax': [0.9695875693160814, 1.0000000000000000],\n",
        "  'image/object/bbox/ymin': [0.5820120073891626, 0.1829972290640394],\n",
        "  'image/object/bbox/ymax': [1.0000000000000000, 0.9662484605911330],\n",
        "  'image/object/class/text': (['Cat', 'Dog']),\n",
        "  'image/object/class/label': ([1, 2])\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAkOvP-gZR1x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import io\n",
        "import json\n",
        "import random\n",
        "\n",
        "import PIL.Image\n",
        "import tensorflow as tf\n",
        "\n",
        "from object_detection.utils import dataset_util\n",
        "from object_detection.utils import label_map_util\n",
        "\n",
        "def create_tf_record(annotations, label_map, image_path, output):\n",
        "  # Create a train.record TFRecord file.\n",
        "  with tf.python_io.TFRecordWriter(output) as writer:\n",
        "    # Loop through all the training examples.\n",
        "    for annotation in annotations:\n",
        "      try:\n",
        "        # Make sure the image is actually a file\n",
        "        img_path = os.path.join(image_path, annotation['image'])   \n",
        "        if not os.path.isfile(img_path):\n",
        "          continue\n",
        "          \n",
        "        # Read in the image.\n",
        "        with tf.gfile.GFile(img_path, 'rb') as fid:\n",
        "          encoded_jpg = fid.read()\n",
        "\n",
        "        # Open the image with PIL so we can check that it's a jpeg and get the image\n",
        "        # dimensions.\n",
        "        encoded_jpg_io = io.BytesIO(encoded_jpg)\n",
        "        image = PIL.Image.open(encoded_jpg_io)\n",
        "        if image.format != 'JPEG':\n",
        "          raise ValueError('Image format not JPEG')\n",
        "\n",
        "        width, height = image.size\n",
        "\n",
        "        # Initialize all the arrays.\n",
        "        xmins = []\n",
        "        xmaxs = []\n",
        "        ymins = []\n",
        "        ymaxs = []\n",
        "        classes_text = []\n",
        "        classes = []\n",
        "\n",
        "        # The class text is the label name and the class is the id. If there are 3\n",
        "        # cats in the image and 1 dog, it may look something like this:\n",
        "        # classes_text = ['Cat', 'Cat', 'Dog', 'Cat']\n",
        "        # classes      = [  1  ,   1  ,   2  ,   1  ]\n",
        "\n",
        "        # For each image, loop through all the annotations and append their values.\n",
        "        for a in annotation['annotations']:\n",
        "          coord = a['coordinates']\n",
        "          xmin = coord['x'] - (coord['width'] / 2)\n",
        "          xmax = xmin + coord['width']\n",
        "          ymin = coord['y'] - (coord['height'] / 2)\n",
        "          ymax = ymin + coord['height']\n",
        "          xmins.append(max(xmin / width, 0))\n",
        "          xmaxs.append(min(xmax / width, 1))\n",
        "          ymins.append(max(ymin / height, 0))\n",
        "          ymaxs.append(min(ymax / height, 1))\n",
        "          label = a['label']\n",
        "          classes_text.append(label.encode('utf8'))\n",
        "          classes.append(label_map[label])\n",
        "      \n",
        "        # Create the TFExample.\n",
        "        tf_example = tf.train.Example(features=tf.train.Features(feature={\n",
        "          'image/height': dataset_util.int64_feature(height),\n",
        "          'image/width': dataset_util.int64_feature(width),\n",
        "          'image/filename': dataset_util.bytes_feature(annotation['image'].encode('utf8')),\n",
        "          'image/source_id': dataset_util.bytes_feature(annotation['image'].encode('utf8')),\n",
        "          'image/encoded': dataset_util.bytes_feature(encoded_jpg),\n",
        "          'image/format': dataset_util.bytes_feature('jpeg'.encode('utf8')),\n",
        "          'image/object/bbox/xmin': dataset_util.float_list_feature(xmins),\n",
        "          'image/object/bbox/xmax': dataset_util.float_list_feature(xmaxs),\n",
        "          'image/object/bbox/ymin': dataset_util.float_list_feature(ymins),\n",
        "          'image/object/bbox/ymax': dataset_util.float_list_feature(ymaxs),\n",
        "          'image/object/class/text': dataset_util.bytes_list_feature(classes_text),\n",
        "          'image/object/class/label': dataset_util.int64_list_feature(classes),\n",
        "        }))\n",
        "        if tf_example:\n",
        "          # Write the TFExample to the TFRecord.\n",
        "          writer.write(tf_example.SerializeToString())\n",
        "      except ValueError:\n",
        "        print('Invalid example, ignoring.')\n",
        "        pass\n",
        "      except IOError:\n",
        "        # print(\"Can't read example, ignoring.\")\n",
        "        pass\n",
        "\n",
        "with open(ANNOTATIONS_JSON_PATH) as f:\n",
        "  annotations = json.load(f)\n",
        "  # Load the label map we created.\n",
        "  label_map = label_map_util.get_label_map_dict(LABEL_MAP_PATH)\n",
        "\n",
        "  random.seed(42)\n",
        "  random.shuffle(annotations)\n",
        "  num_train = int(0.7 * len(annotations))\n",
        "  train_examples = annotations[:num_train]\n",
        "  val_examples = annotations[num_train:]\n",
        "\n",
        "  create_tf_record(train_examples, label_map, ANNOTATIONS_PATH, TRAIN_RECORD_PATH)\n",
        "  create_tf_record(val_examples, label_map, ANNOTATIONS_PATH, VAL_RECORD_PATH)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6DhYpAS7gX2",
        "colab_type": "text"
      },
      "source": [
        "# Download a base model\n",
        "Training a model from scratch can take days and tons of data. We can mitigate this by using a pretrained model checkpoint. Instead of starting from nothing, we can add to what was already learned with our own data.\n",
        "\n",
        "We can get a download a checkpoint from the [model zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md).\n",
        "\n",
        "We can download the checkpoint to our training run with the following code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHD1Jm0v7jfz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import tarfile\n",
        "\n",
        "import six.moves.urllib as urllib\n",
        "\n",
        "download_base = 'http://download.tensorflow.org/models/object_detection/'\n",
        "model = 'ssd_mobilenet_v1_quantized_300x300_coco14_sync_2018_07_18.tar.gz'\n",
        "tmp = '/content/checkpoint.tar.gz'\n",
        "\n",
        "if not (os.path.exists(CHECKPOINT_PATH)):\n",
        "  # Download the checkpoint\n",
        "  opener = urllib.request.URLopener()\n",
        "  opener.retrieve(download_base + model, tmp)\n",
        "\n",
        "  # Extract all the `model.ckpt` files.\n",
        "  with tarfile.open(tmp) as tar:\n",
        "    for member in tar.getmembers():\n",
        "      member.name = os.path.basename(member.name)\n",
        "      if 'model.ckpt' in member.name:\n",
        "        tar.extract(member, path=CHECKPOINT_PATH)\n",
        "\n",
        "  os.remove(tmp)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXlvFvwUHrui",
        "colab_type": "text"
      },
      "source": [
        "# Model config\n",
        "The final thing we need to do is inject our pipline with the amount of labels we have and where to find the label map, TFRecord and model checkpoint. We also need to change the the batch size, because the default batch size of 128 is too large for Colab to handle."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8CVExv6HsJS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "\n",
        "from google.protobuf import text_format\n",
        "\n",
        "from object_detection.utils import config_util\n",
        "from object_detection.utils import label_map_util\n",
        "\n",
        "pipeline_skeleton = '/content/models/research/object_detection/samples/configs/ssd_mobilenet_v1_quantized_300x300_coco14_sync.config'\n",
        "configs = config_util.get_configs_from_pipeline_file(pipeline_skeleton)\n",
        "\n",
        "label_map = label_map_util.get_label_map_dict(LABEL_MAP_PATH)\n",
        "num_classes = len(label_map.keys())\n",
        "meta_arch = configs[\"model\"].WhichOneof(\"model\")\n",
        "\n",
        "override_dict = {\n",
        "  'model.{}.num_classes'.format(meta_arch): num_classes,\n",
        "  'train_config.batch_size': 24,\n",
        "  'train_input_path': TRAIN_RECORD_PATH,\n",
        "  'eval_input_path': VAL_RECORD_PATH,\n",
        "  'train_config.fine_tune_checkpoint': os.path.join(CHECKPOINT_PATH, 'model.ckpt'),\n",
        "  'label_map_path': LABEL_MAP_PATH\n",
        "}\n",
        "\n",
        "configs = config_util.merge_external_params_with_configs(configs, kwargs_dict=override_dict)\n",
        "pipeline_config = config_util.create_pipeline_proto_from_configs(configs)\n",
        "config_util.save_pipeline_config(pipeline_config, DATA_PATH)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNYIZK1xVNAa",
        "colab_type": "text"
      },
      "source": [
        "# Start training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wv5h2bwBVO0V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -rf $OUTPUT_PATH\n",
        "!python -m object_detection.model_main \\\n",
        "    --pipeline_config_path=$DATA_PATH/pipeline.config \\\n",
        "    --model_dir=$OUTPUT_PATH \\\n",
        "    --num_train_steps=$NUM_TRAIN_STEPS \\\n",
        "    --num_eval_steps=100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwNtvgtdoB-C",
        "colab_type": "text"
      },
      "source": [
        "# Export inference graph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZgP_FZUoE0d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import re\n",
        "\n",
        "regex = re.compile(r\"model\\.ckpt-([0-9]+)\\.index\")\n",
        "numbers = [int(regex.search(f).group(1)) for f in os.listdir(OUTPUT_PATH) if regex.search(f)]\n",
        "TRAINED_CHECKPOINT_PREFIX = os.path.join(OUTPUT_PATH, 'model.ckpt-{}'.format(max(numbers)))\n",
        "\n",
        "!python -m object_detection.export_inference_graph \\\n",
        "  --pipeline_config_path=$DATA_PATH/pipeline.config \\\n",
        "  --trained_checkpoint_prefix=$TRAINED_CHECKPOINT_PREFIX \\\n",
        "  --output_directory=$EXPORTED_PATH"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVmCc9NPkP3U",
        "colab_type": "text"
      },
      "source": [
        "# Convert the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-f5lfcnp01e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!tensorflowjs_converter \\\n",
        "  --input_format=tf_frozen_model \\\n",
        "  --output_format=tfjs_graph_model \\\n",
        "  --output_node_names='Postprocessor/ExpandDims_1,Postprocessor/Slice' \\\n",
        "  --quantization_bytes=1 \\\n",
        "  --skip_op_check \\\n",
        "  $EXPORTED_PATH/frozen_inference_graph.pb \\\n",
        "  /content/model_web\n",
        "\n",
        "import json\n",
        "\n",
        "from object_detection.utils.label_map_util import get_label_map_dict\n",
        "\n",
        "label_map = get_label_map_dict(LABEL_MAP_PATH)\n",
        "label_array = [k for k in sorted(label_map, key=label_map.get)]\n",
        "\n",
        "with open(os.path.join('/content/model_web', 'labels.json'), 'w') as f:\n",
        "  json.dump(label_array, f)\n",
        "\n",
        "!cd /content/model_web && zip -r /content/model_web.zip *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1iFI9pPr1l7",
        "colab_type": "text"
      },
      "source": [
        "# Download the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FL_miSj2r1yt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "files.download('/content/model_web.zip') "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}